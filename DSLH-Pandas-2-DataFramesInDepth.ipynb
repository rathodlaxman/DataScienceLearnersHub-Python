{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# **`Data Science Learners Hub`**\n",
                "\n",
                "**Module : Python**\n",
                "\n",
                "**email** : [datasciencelearnershub@gmail.com](mailto:datasciencelearnershub@gmail.com)"
            ],
            "metadata": {
                "azdata_cell_guid": "882cccd6-2a88-4dad-a29f-7c371f52fd3b"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## **`#2: DataFrames in Depth`**\n",
                "4. **Creating DataFrames**\n",
                "   - From lists, dictionaries, and arrays\n",
                "   - Reading data from CSV, Excel, and other formats\n",
                "\n",
                "5. **Basic DataFrame Operations**\n",
                "   - Inspecting the DataFrame\n",
                "   - Indexing and selecting data\n",
                "   - Descriptive statistics\n",
                "\n",
                "6. **Data Cleaning and Handling Missing Data**\n",
                "   - Handling missing values\n",
                "   - Dropping or filling missing values\n",
                "   - Removing duplicates"
            ],
            "metadata": {
                "azdata_cell_guid": "6e465506-225c-465a-ad95-ea27ecf4f724"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **`4. Creating DataFrames: `**"
            ],
            "metadata": {
                "azdata_cell_guid": "57ed9944-763e-4f8a-9521-47f18cd5207b"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "\n",
                "#### `From Lists, Dictionaries, and Arrays`\n",
                "\n",
                "**Introduction:**\n",
                "Creating a Pandas DataFrame is a fundamental step in data analysis. In this prompt, we will explore three common methods for creating DataFrames: using lists, dictionaries, and arrays.\n",
                "\n",
                "**From Lists:**\n",
                "\n",
                "1. **Using Lists as Columns:**\n",
                "   - You can create a DataFrame by using lists as columns. Each list represents a column, and the lengths of the lists must match.\n",
                "     ```python\n",
                "     import pandas as pd\n",
                "\n",
                "     names = ['Alice', 'Bob', 'Charlie']\n",
                "     ages = [25, 30, 35]\n",
                "     cities = ['New York', 'San Francisco', 'Los Angeles']\n",
                "\n",
                "     df_from_lists = pd.DataFrame({'Name': names, 'Age': ages, 'City': cities})\n",
                "     ```\n",
                "\n",
                "2. **Specifying Index:**\n",
                "   - You can specify a custom index for the DataFrame:\n",
                "     ```python\n",
                "     custom_index = ['person1', 'person2', 'person3']\n",
                "     df_from_lists_custom_index = pd.DataFrame({'Name': names, 'Age': ages, 'City': cities}, index=custom_index)\n",
                "     ```\n",
                "\n",
                "**From Dictionaries:**\n",
                "\n",
                "1. **Using Dictionary Keys as Columns:**\n",
                "   - Creating a DataFrame from a dictionary allows you to use the keys as column names.\n",
                "     ```python\n",
                "     data_dict = {'Name': ['Alice', 'Bob', 'Charlie'],\n",
                "                  'Age': [25, 30, 35],\n",
                "                  'City': ['New York', 'San Francisco', 'Los Angeles']}\n",
                "     \n",
                "     df_from_dict = pd.DataFrame(data_dict)\n",
                "     ```\n",
                "\n",
                "2. **Specifying Index:**\n",
                "   - Similar to the list method, you can specify a custom index:\n",
                "     ```python\n",
                "     df_from_dict_custom_index = pd.DataFrame(data_dict, index=custom_index)\n",
                "     ```\n",
                "\n",
                "**From Arrays:**\n",
                "\n",
                "1. **Using NumPy Arrays:**\n",
                "   - NumPy arrays can be used to create DataFrames. Ensure that the dimensions match for each array.\n",
                "     ```python\n",
                "     import numpy as np\n",
                "\n",
                "     names_array = np.array(['Alice', 'Bob', 'Charlie'])\n",
                "     ages_array = np.array([25, 30, 35])\n",
                "     cities_array = np.array(['New York', 'San Francisco', 'Los Angeles'])\n",
                "\n",
                "     df_from_arrays = pd.DataFrame({'Name': names_array, 'Age': ages_array, 'City': cities_array})\n",
                "     ```\n",
                "\n",
                "2. **Specifying Index:**\n",
                "   - As before, you can specify a custom index:\n",
                "     ```python\n",
                "     df_from_arrays_custom_index = pd.DataFrame({'Name': names_array, 'Age': ages_array, 'City': cities_array}, index=custom_index)\n",
                "     ```\n",
                "\n",
                "**Importance of Specifying Column Names and Indices:**\n",
                "\n",
                "1. **Clarity and Readability:**\n",
                "   - Specifying meaningful column names enhances the clarity and readability of your code and data.\n",
                "\n",
                "2. **Consistency in Analysis:**\n",
                "   - A consistent index allows for smoother and more predictable data analysis, especially when combining DataFrames or performing complex operations.\n",
                "\n",
                "3. **Avoiding Ambiguity:**\n",
                "   - Explicitly defining column names and indices avoids ambiguity and ensures that each piece of data is correctly associated with its intended category.\n",
                "\n",
                "**Conclusion:**\n",
                "Creating DataFrames in Pandas using lists, dictionaries, and arrays provides flexibility and versatility in handling different types of data. Specifying column names and indices during DataFrame creation is essential for clarity and consistency in subsequent data analysis tasks.\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "427a9a8a-b73b-4c22-acfa-d9b3a8d933e2"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Example :"
            ],
            "metadata": {
                "azdata_cell_guid": "18ab505e-a8c2-45ff-b3d6-cd3b095c060a"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# Creating a DataFrame from Lists\n",
                "list_data = {\n",
                "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emma'],\n",
                "    'Age': [25, 30, 35, 22, 28],\n",
                "    'Salary': [50000, 60000, 75000, 48000, 55000],\n",
                "    'Experience': [3, 5, 8, 2, 4],\n",
                "}\n",
                "\n",
                "df_from_lists = pd.DataFrame(list_data)\n",
                "\n",
                "# Creating a DataFrame from a Dictionary\n",
                "dict_data = {\n",
                "    'ID': [1, 2, 3, 4, 5],\n",
                "    'Subject': ['Math', 'Physics', 'Chemistry', 'Biology', 'English'],\n",
                "    'Score': [85, 92, 78, 89, 95],\n",
                "}\n",
                "\n",
                "df_from_dict = pd.DataFrame(dict_data)\n",
                "\n",
                "# Creating a DataFrame from Arrays (using NumPy)\n",
                "array_data = np.array([\n",
                "    [1, 'Apple', 3],\n",
                "    [2, 'Banana', 6],\n",
                "    [3, 'Orange', 4],\n",
                "])\n",
                "\n",
                "df_from_arrays = pd.DataFrame(array_data, columns=['ID', 'Fruit', 'Quantity'])\n",
                "\n",
                "# Displaying the DataFrames\n",
                "print(\"DataFrame from Lists:\")\n",
                "print(df_from_lists)\n",
                "\n",
                "print(\"\\nDataFrame from Dictionary:\")\n",
                "print(df_from_dict)\n",
                "\n",
                "print(\"\\nDataFrame from Arrays:\")\n",
                "print(df_from_arrays)\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "45896f83-3c0d-4711-8099-3aa9f8d997f3",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "DataFrame from Lists:\n      Name  Age  Salary  Experience\n0    Alice   25   50000           3\n1      Bob   30   60000           5\n2  Charlie   35   75000           8\n3    David   22   48000           2\n4     Emma   28   55000           4\n\nDataFrame from Dictionary:\n   ID    Subject  Score\n0   1       Math     85\n1   2    Physics     92\n2   3  Chemistry     78\n3   4    Biology     89\n4   5    English     95\n\nDataFrame from Arrays:\n  ID   Fruit Quantity\n0  1   Apple        3\n1  2  Banana        6\n2  3  Orange        4\n"
                }
            ],
            "execution_count": 1
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Real World Scenario:\n",
                "Imagine you have survey data from a group of people regarding their preferences for various types of electronic devices. Each person's data includes their ID, name, age, and the ratings (out of 10) they gave to different devices like smartphones, laptops, and smartwatches."
            ],
            "metadata": {
                "azdata_cell_guid": "c41f97f0-dd26-4391-a6eb-3dce4629c4c1"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Sample survey data\n",
                "survey_data = {\n",
                "    'ID': [1, 2, 3, 4, 5],\n",
                "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emma'],\n",
                "    'Age': [25, 30, 35, 22, 28],\n",
                "    'Smartphone_Rating': [9, 8, 7, 9, 8],\n",
                "    'Laptop_Rating': [8, 7, 9, 6, 8],\n",
                "    'Smartwatch_Rating': [7, 6, 8, 7, 9],\n",
                "}\n",
                "\n",
                "# Creating a DataFrame from the survey data\n",
                "df_survey = pd.DataFrame(survey_data)\n",
                "\n",
                "# Displaying the DataFrame\n",
                "print(\"Survey Data DataFrame:\")\n",
                "print(df_survey)"
            ],
            "metadata": {
                "azdata_cell_guid": "c3f2c0ff-0c68-423e-994b-2bb29ae39eb1",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Survey Data DataFrame:\n   ID     Name  Age  Smartphone_Rating  Laptop_Rating  Smartwatch_Rating\n0   1    Alice   25                  9              8                  7\n1   2      Bob   30                  8              7                  6\n2   3  Charlie   35                  7              9                  8\n3   4    David   22                  9              6                  7\n4   5     Emma   28                  8              8                  9\n"
                }
            ],
            "execution_count": 2
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Considerations or Peculiarities:\n",
                "\n",
                "- **Column Consistency:** Ensure consistency in the length of lists or arrays when creating a DataFrame. All lists should have the same length, or dictionaries should have the same set of keys.\n",
                "\n",
                "- **Data Types:** Be mindful of the data types within lists or arrays. Pandas will attempt to infer data types, but it's helpful to explicitly specify them if needed.\n",
                "\n",
                "- **Indexing:** Decide whether you need to set a specific column as the index. In the example above, 'ID' is set as the index, but you may choose another column or leave it with the default integer index."
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "4243b440-4f80-4cc6-bd66-4f2ee24002cb"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Common Mistakes:\n",
                "\n",
                "- **Mismatched Lengths:** Forgetting to check and ensure that all lists or arrays used to create a DataFrame have the same length can lead to errors.\n",
                "\n",
                "- **Misspelled Column Names:** When creating a DataFrame from a dictionary, ensure that the keys represent column names. Misspelling a key may result in the creation of a new column instead of using an existing one.\n",
                "\n",
                "- **Incorrect Data Types:** If your data types are not appropriate, it can lead to unexpected results. Check that numeric columns are treated as numbers, and categorical columns are specified as such."
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "83470cec-4f95-4b62-a8ef-411d6bff6173"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "\n",
                "#### `Reading Data into a DataFrame from Various Formats`\n",
                "\n",
                "**Introduction:**\n",
                "Pandas provides versatile functions to read data from different file formats, making it a powerful tool for handling diverse data sources. In this prompt, we will explore how to read data into a DataFrame from common formats such as CSV and Excel, and discuss additional formats supported by Pandas.\n",
                "\n",
                "**Reading from CSV:**\n",
                "\n",
                "1. **Using `read_csv` Function:**\n",
                "   - Reading data from a CSV file is straightforward using the `read_csv` function:\n",
                "     ```python\n",
                "     import pandas as pd\n",
                "\n",
                "     df_csv = pd.read_csv('data.csv')\n",
                "     ```\n",
                "\n",
                "2. **Customizing Parameters:**\n",
                "   - You can customize parameters such as delimiter, encoding, and header during reading:\n",
                "     ```python\n",
                "     df_custom_csv = pd.read_csv('data.csv', delimiter=';', encoding='utf-8', header=0)\n",
                "     ```\n",
                "\n",
                "**Reading from Excel:**\n",
                "\n",
                "1. **Using `read_excel` Function:**\n",
                "   - Reading data from an Excel file is accomplished with the `read_excel` function:\n",
                "     ```python\n",
                "     df_excel = pd.read_excel('data.xlsx', sheet_name='Sheet1')\n",
                "     ```\n",
                "\n",
                "2. **Specifying Columns:**\n",
                "   - You can specify columns to read from Excel:\n",
                "     ```python\n",
                "     df_excel_columns = pd.read_excel('data.xlsx', sheet_name='Sheet1', usecols=['Name', 'Age'])\n",
                "     ```\n",
                "\n",
                "**Reading from Other Formats:**\n",
                "\n",
                "1. **JSON:**\n",
                "   - Pandas supports reading data from JSON files:\n",
                "     ```python\n",
                "     df_json = pd.read_json('data.json')\n",
                "     ```\n",
                "\n",
                "2. **HTML (Web Scraping):**\n",
                "   - Reading tables from HTML pages (web scraping) is possible using `read_html`:\n",
                "     ```python\n",
                "     url = 'https://example.com/table'\n",
                "     df_html = pd.read_html(url)[0]  # [0] selects the first table from the page\n",
                "     ```\n",
                "\n",
                "3. **SQL Databases:**\n",
                "   - Reading data from SQL databases using `read_sql`:\n",
                "     ```python\n",
                "     from sqlalchemy import create_engine\n",
                "\n",
                "     engine = create_engine('sqlite:///example.db')\n",
                "     query = 'SELECT * FROM my_table'\n",
                "     df_sql = pd.read_sql(query, engine)\n",
                "     ```\n",
                "\n",
                "**Flexibility in Handling Diverse Data Sources:**\n",
                "\n",
                "1. **URLs and HTTP(S):**\n",
                "   - Reading data directly from URLs:\n",
                "     ```python\n",
                "     url = 'https://example.com/data.csv'\n",
                "     df_url = pd.read_csv(url)\n",
                "     ```\n",
                "\n",
                "2. **ZIP Archives:**\n",
                "   - Reading data from files within a ZIP archive:\n",
                "     ```python\n",
                "     df_zip = pd.read_csv('archive.zip', compression='zip', header=0)\n",
                "     ```\n",
                "\n",
                "3. **Reading from Clipboard:**\n",
                "   - Copying data to the clipboard and reading it directly into a DataFrame:\n",
                "     ```python\n",
                "     df_clipboard = pd.read_clipboard()\n",
                "     ```\n",
                "\n",
                "**Conclusion:**\n",
                "Pandas' flexibility in reading data from various formats, including CSV, Excel, JSON, HTML, SQL databases, and more, makes it a versatile tool for handling diverse data sources. The ability to read directly from URLs, ZIP archives, and the clipboard enhances its capabilities for real-world data scenarios.\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "5edb63fd-7981-4776-a79d-a1306f82fa03"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **`5. Basic DataFrame Operations`**"
            ],
            "metadata": {
                "azdata_cell_guid": "67065aaa-6b6d-45fd-bb0f-2c66dd49b2ce"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "\n",
                "#### **`Inspecting the DataFrame`**\n",
                "\n",
                "**Introduction:**\n",
                "Inspecting a DataFrame is an essential step in understanding its structure and contents. Pandas provides several methods that allow you to gain insights into the data quickly. In this prompt, we'll explore common methods such as `head()`, `tail()`, `info()`, `shape`, and `describe()`.\n",
                "\n",
                "**Using `head()` and `tail()`:**\n",
                "\n",
                "1. **`head(n)`:**\n",
                "   - The `head()` method displays the first `n` rows of the DataFrame. It is useful for quickly getting an overview of the dataset.\n",
                "     ```python\n",
                "     import pandas as pd\n",
                "\n",
                "     df = pd.read_csv('data.csv')\n",
                "     df_head = df.head(5)  # Display the first 5 rows\n",
                "     ```\n",
                "\n",
                "2. **`tail(n)`:**\n",
                "   - The `tail()` method shows the last `n` rows of the DataFrame, allowing you to inspect the end of the dataset.\n",
                "     ```python\n",
                "     df_tail = df.tail(5)  # Display the last 5 rows\n",
                "     ```\n",
                "\n",
                "**Using `info()`:**\n",
                "\n",
                "1. **`info()`:**\n",
                "   - The `info()` method provides a concise summary of the DataFrame, including the data types, non-null counts, and memory usage.\n",
                "     ```python\n",
                "     df_info = df.info()\n",
                "     ```\n",
                "\n",
                "**Using `shape`:**\n",
                "\n",
                "1. **`shape`:**\n",
                "   - The `shape` attribute returns a tuple representing the dimensions of the DataFrame (number of rows, number of columns).\n",
                "     ```python\n",
                "     df_shape = df.shape\n",
                "     ```\n",
                "\n",
                "**Using `describe()`:**\n",
                "\n",
                "1. **`describe()`:**\n",
                "   - The `describe()` method generates descriptive statistics, including measures of central tendency, dispersion, and shape of the distribution.\n",
                "     ```python\n",
                "     df_describe = df.describe()\n",
                "     ```\n",
                "\n",
                "2. **Customizing `describe()`:**\n",
                "   - You can customize the output of `describe()` to include specific percentiles or types of statistics.\n",
                "     ```python\n",
                "     custom_describe = df.describe(percentiles=[0.25, 0.5, 0.75], include='all')\n",
                "     ```\n",
                "\n",
                "**Conclusion:**\n",
                "Inspecting a DataFrame is a crucial step in the data analysis process. Methods such as `head()`, `tail()`, `info()`, `shape`, and `describe()` provide valuable information about the structure, contents, and statistical summary of the dataset. Using these methods allows you to quickly assess the data and make informed decisions about further analysis.\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "af575830-e61c-4d28-9651-62739d164f3b"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Example:\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "f6fc3fae-bba9-4d21-acda-135c6f5c1518"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Creating a sample DataFrame\n",
                "data = {\n",
                "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emma'],\n",
                "    'Age': [25, 30, 35, 22, 28],\n",
                "    'Salary': [50000, 60000, 75000, 48000, 55000],\n",
                "    'Experience': [3, 5, 8, 2, 4],\n",
                "}\n",
                "\n",
                "df = pd.DataFrame(data)\n",
                "\n",
                "# Displaying the DataFrame\n",
                "print(\"Original DataFrame:\")\n",
                "print(df)\n",
                "\n",
                "# Using head() and tail() for an overview\n",
                "print(\"\\nFirst 3 Rows (head()):\")\n",
                "print(df.head(3))\n",
                "\n",
                "print(\"\\nLast 2 Rows (tail()):\")\n",
                "print(df.tail(2))\n",
                "\n",
                "# Using info() for a summary\n",
                "print(\"\\nDataFrame Info:\")\n",
                "df_info = df.info()\n",
                "\n",
                "# Using shape to get dimensions\n",
                "print(\"\\nDataFrame Shape:\")\n",
                "df_shape = df.shape\n",
                "\n",
                "# Using describe() for summary statistics\n",
                "print(\"\\nSummary Statistics:\")\n",
                "df_describe = df.describe()\n",
                "\n",
                "# Displaying the results\n",
                "print(\"\\nResults:\")\n",
                "print(df_info)\n",
                "print(\"\\nDataFrame Shape:\", df_shape)\n",
                "print(\"\\nSummary Statistics:\\n\", df_describe)\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "48a77f37-1012-4f3c-a330-aad3b53049b3",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Original DataFrame:\n      Name  Age  Salary  Experience\n0    Alice   25   50000           3\n1      Bob   30   60000           5\n2  Charlie   35   75000           8\n3    David   22   48000           2\n4     Emma   28   55000           4\n\nFirst 3 Rows (head()):\n      Name  Age  Salary  Experience\n0    Alice   25   50000           3\n1      Bob   30   60000           5\n2  Charlie   35   75000           8\n\nLast 2 Rows (tail()):\n    Name  Age  Salary  Experience\n3  David   22   48000           2\n4   Emma   28   55000           4\n\nDataFrame Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5 entries, 0 to 4\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   Name        5 non-null      object\n 1   Age         5 non-null      int64 \n 2   Salary      5 non-null      int64 \n 3   Experience  5 non-null      int64 \ndtypes: int64(3), object(1)\nmemory usage: 288.0+ bytes\n\nDataFrame Shape:\n\nSummary Statistics:\n\nResults:\nNone\n\nDataFrame Shape: (5, 4)\n\nSummary Statistics:\n              Age        Salary  Experience\ncount   5.000000      5.000000    5.000000\nmean   28.000000  57600.000000    4.400000\nstd     4.949747  10784.247772    2.302173\nmin    22.000000  48000.000000    2.000000\n25%    25.000000  50000.000000    3.000000\n50%    28.000000  55000.000000    4.000000\n75%    30.000000  60000.000000    5.000000\nmax    35.000000  75000.000000    8.000000\n"
                }
            ],
            "execution_count": 3
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Real-world Scenario:\n",
                "Consider a scenario where you have a dataset containing information about sales transactions for an e-commerce platform. You want to inspect the data to understand its structure, check for missing values, and get a quick overview of the sales performance."
            ],
            "metadata": {
                "azdata_cell_guid": "bc4b3d2c-9d4b-4330-8e3e-7262a819e3a5"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Sample e-commerce sales data\n",
                "sales_data = {\n",
                "    'OrderID': [101, 102, 103, 104, 105],\n",
                "    'Product': ['Laptop', 'Smartphone', 'Tablet', 'Headphones', 'Camera'],\n",
                "    'Quantity': [2, 1, 3, 2, 1],\n",
                "    'Price': [1200, 800, 300, 150, 700],\n",
                "    'CustomerID': [101, 102, 103, 104, 105],\n",
                "    'Date': ['2022-01-01', '2022-01-02', '2022-01-02', '2022-01-03', '2022-01-03'],\n",
                "}\n",
                "\n",
                "# Creating a DataFrame from the sales data\n",
                "df_sales = pd.DataFrame(sales_data)\n",
                "\n",
                "# Inspecting the DataFrame\n",
                "print(\"Overview of Sales Data:\")\n",
                "print(df_sales.head())\n",
                "print(\"\\nStructure of Sales Data:\")\n",
                "print(df_sales.info())\n",
                "print(\"\\nSummary Statistics of Sales Data:\")\n",
                "print(df_sales.describe())"
            ],
            "metadata": {
                "azdata_cell_guid": "3422c438-e130-4a14-af7c-e5b85d9376fe",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Overview of Sales Data:\n   OrderID     Product  Quantity  Price  CustomerID        Date\n0      101      Laptop         2   1200         101  2022-01-01\n1      102  Smartphone         1    800         102  2022-01-02\n2      103      Tablet         3    300         103  2022-01-02\n3      104  Headphones         2    150         104  2022-01-03\n4      105      Camera         1    700         105  2022-01-03\n\nStructure of Sales Data:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5 entries, 0 to 4\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   OrderID     5 non-null      int64 \n 1   Product     5 non-null      object\n 2   Quantity    5 non-null      int64 \n 3   Price       5 non-null      int64 \n 4   CustomerID  5 non-null      int64 \n 5   Date        5 non-null      object\ndtypes: int64(4), object(2)\nmemory usage: 368.0+ bytes\nNone\n\nSummary Statistics of Sales Data:\n          OrderID  Quantity        Price  CustomerID\ncount    5.000000   5.00000     5.000000    5.000000\nmean   103.000000   1.80000   630.000000  103.000000\nstd      1.581139   0.83666   417.731971    1.581139\nmin    101.000000   1.00000   150.000000  101.000000\n25%    102.000000   1.00000   300.000000  102.000000\n50%    103.000000   2.00000   700.000000  103.000000\n75%    104.000000   2.00000   800.000000  104.000000\nmax    105.000000   3.00000  1200.000000  105.000000\n"
                }
            ],
            "execution_count": 4
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Considerations or Peculiarities:\n",
                "\n",
                "- **Data Types:** Ensure that data types are appropriate for each column. Dates should be in datetime format, and numerical columns should have the correct data type.\n",
                "\n",
                "- **Missing Values:** Check for missing values using methods like `isnull()` or `info()`. Decide on a strategy to handle missing data if needed.\n",
                "\n",
                "- **Categorical Columns:** Identify and encode categorical columns appropriately. Some columns may have a finite set of categories, and using the `astype('category')` method can save memory.\n",
                "\n",
                "#### Common Mistakes:\n",
                "\n",
                "- **Neglecting Missing Values:** Ignoring missing values during inspection can lead to incorrect analyses. Always check for missing data and decide how to handle it.\n",
                "\n",
                "- **Not Understanding Data Types:** Misinterpreting data types may lead to errors in analysis. Make sure to understand the meaning and representation of each column's data type.\n",
                "\n",
                "- **Overlooking Categorical Variables:** Categorical variables may not always be automatically identified. Check and convert categorical columns if needed, especially if they are nominal or ordinal.\n",
                "\n",
                "Inspecting the DataFrame is a crucial step to understand the data's characteristics and make informed decisions during data analysis. Adapt the example code and considerations based on the specifics of your real-world datasets."
            ],
            "metadata": {
                "azdata_cell_guid": "57ecb78a-2ab4-459a-93c4-f1337c8a31da"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "\n",
                "#### **`Indexing and Selecting Data in a DataFrame`**\n",
                "\n",
                "**Introduction:**\n",
                "Indexing and selecting data in a Pandas DataFrame are fundamental operations for extracting specific subsets of information. Two main methods for this purpose are `loc[]` and `iloc[]`. In this prompt, we'll explore these methods and provide examples of conditional indexing and boolean indexing.\n",
                "\n",
                "**Using `loc[]` for Label-Based Indexing:**\n",
                "\n",
                "1. **Selecting Rows by Label:**\n",
                "   - Use `loc[]` to select rows based on their labels (index values):\n",
                "     ```python\n",
                "     import pandas as pd\n",
                "\n",
                "     df = pd.read_csv('data.csv', index_col='ID')\n",
                "     selected_row = df.loc[2]  # Select row with index 2\n",
                "     ```\n",
                "\n",
                "2. **Selecting Specific Columns for a Row:**\n",
                "   - Specify both row label and column label to select a specific value:\n",
                "     ```python\n",
                "     specific_value = df.loc[2, 'Name']  # Select 'Name' for row with index 2\n",
                "     ```\n",
                "\n",
                "3. **Slicing Rows:**\n",
                "   - Use slicing with labels to select a range of rows:\n",
                "     ```python\n",
                "     sliced_rows = df.loc[2:5]  # Select rows with indices 2 to 5 (inclusive)\n",
                "     ```\n",
                "\n",
                "4. **Selecting Rows and Columns Simultaneously:**\n",
                "   - Use `loc[]` to select specific rows and columns:\n",
                "     ```python\n",
                "     selected_data = df.loc[2:5, ['Name', 'Age']]\n",
                "     ```\n",
                "\n",
                "**Using `iloc[]` for Position-Based Indexing:**\n",
                "\n",
                "1. **Selecting Rows by Position:**\n",
                "   - Use `iloc[]` to select rows based on their integer positions:\n",
                "     ```python\n",
                "     selected_row_position = df.iloc[1]  # Select the second row (position 1)\n",
                "     ```\n",
                "\n",
                "2. **Selecting Specific Columns for a Row by Position:**\n",
                "   - Specify both row position and column position to select a specific value:\n",
                "     ```python\n",
                "     specific_value_position = df.iloc[1, 0]  # Select the first column for the second row\n",
                "     ```\n",
                "\n",
                "3. **Slicing Rows by Position:**\n",
                "   - Use slicing with integer positions to select a range of rows:\n",
                "     ```python\n",
                "     sliced_rows_position = df.iloc[1:4]  # Select rows with positions 1 to 3\n",
                "     ```\n",
                "\n",
                "4. **Selecting Rows and Columns Simultaneously by Position:**\n",
                "   - Use `iloc[]` to select specific rows and columns by position:\n",
                "     ```python\n",
                "     selected_data_position = df.iloc[1:4, [0, 1]]\n",
                "     ```\n",
                "\n",
                "**Conditional Indexing and Boolean Indexing:**\n",
                "\n",
                "1. **Conditional Indexing:**\n",
                "   - Use boolean conditions to filter rows based on a specific criterion:\n",
                "     ```python\n",
                "     condition = df['Age'] > 30\n",
                "     conditionally_selected = df.loc[condition]\n",
                "     ```\n",
                "\n",
                "2. **Boolean Indexing:**\n",
                "   - Use boolean arrays directly for filtering:\n",
                "     ```python\n",
                "     boolean_selected = df[df['Age'] > 30]\n",
                "     ```\n",
                "\n",
                "**Conclusion:**\n",
                "Indexing and selecting data in a Pandas DataFrame using `loc[]` and `iloc[]` are powerful techniques. These methods allow you to retrieve specific rows and columns based on labels or positions. Additionally, conditional indexing and boolean indexing enable you to filter data efficiently based on specific criteria.\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "685a43a4-85e4-463b-83db-dfbbef46211a"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Example:\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "b400e568-6b33-46a4-9c62-6272e4eb8989"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Creating a sample DataFrame\n",
                "data = {\n",
                "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emma'],\n",
                "    'Age': [25, 30, 35, 22, 28],\n",
                "    'Salary': [50000, 60000, 75000, 48000, 55000],\n",
                "    'Experience': [3, 5, 8, 2, 4],\n",
                "}\n",
                "\n",
                "df = pd.DataFrame(data)\n",
                "df.set_index('Name', inplace=True)  # Set 'Name' column as the index\n",
                "\n",
                "# Using loc[] for Label-Based Indexing\n",
                "selected_row = df.loc['Bob']\n",
                "specific_value = df.loc['Bob', 'Age']\n",
                "sliced_rows = df.loc['Bob':'David']\n",
                "selected_data = df.loc[['Bob', 'David'], ['Age', 'Salary']]\n",
                "\n",
                "# Using iloc[] for Position-Based Indexing\n",
                "selected_row_position = df.iloc[1]\n",
                "specific_value_position = df.iloc[1, 0]\n",
                "sliced_rows_position = df.iloc[1:4]\n",
                "selected_data_position = df.iloc[1:4, [0, 1]]\n",
                "\n",
                "# Conditional Indexing and Boolean Indexing\n",
                "condition = df['Age'] > 30\n",
                "conditionally_selected = df.loc[condition]\n",
                "boolean_selected = df[df['Age'] > 30]\n",
                "\n",
                "# Displaying the results\n",
                "print(\"Using loc[] for Label-Based Indexing:\")\n",
                "print(\"Selected Row:\\n\", selected_row)\n",
                "print(\"Specific Value:\\n\", specific_value)\n",
                "print(\"Sliced Rows:\\n\", sliced_rows)\n",
                "print(\"Selected Data:\\n\", selected_data)\n",
                "\n",
                "print(\"\\nUsing iloc[] for Position-Based Indexing:\")\n",
                "print(\"Selected Row by Position:\\n\", selected_row_position)\n",
                "print(\"Specific Value by Position:\\n\", specific_value_position)\n",
                "print(\"Sliced Rows by Position:\\n\", sliced_rows_position)\n",
                "print(\"Selected Data by Position:\\n\", selected_data_position)\n",
                "\n",
                "print(\"\\nConditional Indexing and Boolean Indexing:\")\n",
                "print(\"Conditionally Selected:\\n\", conditionally_selected)\n",
                "print(\"Boolean Selected:\\n\", boolean_selected)\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "b5bf48fa-5d23-455c-a23b-048a9609dbb8",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Using loc[] for Label-Based Indexing:\nSelected Row:\n Age              30\nSalary        60000\nExperience        5\nName: Bob, dtype: int64\nSpecific Value:\n 30\nSliced Rows:\n          Age  Salary  Experience\nName                            \nBob       30   60000           5\nCharlie   35   75000           8\nDavid     22   48000           2\nSelected Data:\n        Age  Salary\nName              \nBob     30   60000\nDavid   22   48000\n\nUsing iloc[] for Position-Based Indexing:\nSelected Row by Position:\n Age              30\nSalary        60000\nExperience        5\nName: Bob, dtype: int64\nSpecific Value by Position:\n 30\nSliced Rows by Position:\n          Age  Salary  Experience\nName                            \nBob       30   60000           5\nCharlie   35   75000           8\nDavid     22   48000           2\nSelected Data by Position:\n          Age  Salary\nName                \nBob       30   60000\nCharlie   35   75000\nDavid     22   48000\n\nConditional Indexing and Boolean Indexing:\nConditionally Selected:\n          Age  Salary  Experience\nName                            \nCharlie   35   75000           8\nBoolean Selected:\n          Age  Salary  Experience\nName                            \nCharlie   35   75000           8\n"
                }
            ],
            "execution_count": 5
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Real-world Scenario:\n",
                "Imagine you have a dataset containing information about employees in a company, including their ID, name, department, salary, and performance ratings. You want to perform various operations to analyze and extract specific information about employees."
            ],
            "metadata": {
                "azdata_cell_guid": "52eb0526-240f-41a9-a628-7af193b45060"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Sample employee data\n",
                "employee_data = {\n",
                "    'EmployeeID': [101, 102, 103, 104, 105],\n",
                "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emma'],\n",
                "    'Department': ['HR', 'IT', 'Sales', 'Finance', 'Marketing'],\n",
                "    'Salary': [60000, 75000, 80000, 90000, 70000],\n",
                "    'PerformanceRating': [8.5, 9.2, 7.8, 8.9, 9.5],\n",
                "}\n",
                "\n",
                "# Creating a DataFrame from the employee data\n",
                "df_employees = pd.DataFrame(employee_data)\n",
                "\n",
                "# Indexing and selecting data\n",
                "selected_employee = df_employees.loc[df_employees['Name'] == 'Bob']\n",
                "high_performance_employees = df_employees[df_employees['PerformanceRating'] > 9.0]\n",
                "selected_columns = df_employees.loc[:, ['Name', 'Department', 'Salary']]\n",
                "\n",
                "# Displaying the selected data\n",
                "print(\"Selected Employee (Bob):\\n\", selected_employee)\n",
                "print(\"\\nHigh-Performance Employees:\\n\", high_performance_employees)\n",
                "print(\"\\nSelected Columns:\\n\", selected_columns)\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "98d35ff5-8ea2-48d1-a71a-bc69b18a42f5",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Selected Employee (Bob):\n    EmployeeID Name Department  Salary  PerformanceRating\n1         102  Bob         IT   75000                9.2\n\nHigh-Performance Employees:\n    EmployeeID  Name Department  Salary  PerformanceRating\n1         102   Bob         IT   75000                9.2\n4         105  Emma  Marketing   70000                9.5\n\nSelected Columns:\n       Name Department  Salary\n0    Alice         HR   60000\n1      Bob         IT   75000\n2  Charlie      Sales   80000\n3    David    Finance   90000\n4     Emma  Marketing   70000\n"
                }
            ],
            "execution_count": 6
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Considerations or Peculiarities:\n",
                "\n",
                "- **Indexing Choice:** Choose an appropriate column as the index based on your analysis needs. It could be a unique identifier like employee ID or another column that is relevant to your analysis.\n",
                "\n",
                "- **Boolean Indexing:** Understand how to use boolean indexing effectively. It allows you to filter data based on conditions, as shown in the example with high-performance employees.\n",
                "\n",
                "- **Column Selection:** Be mindful of the columns you select. If you only need specific columns, it's more efficient to select those rather than the entire DataFrame.\n",
                "\n",
                "#### Common Mistakes:\n",
                "\n",
                "- **Incorrect Syntax:** Incorrect use of square brackets, parentheses, or quotation marks in the indexing conditions can lead to errors. Always double-check syntax.\n",
                "\n",
                "- **Using `==` for Float Comparison:** When comparing float values, be cautious due to potential precision issues. Using methods like `np.isclose()` is recommended for float comparisons.\n",
                "\n",
                "- **Misunderstanding Boolean Indexing:** Developers may mistakenly think that boolean indexing is limited to exact matches, but it can be used for various conditions.\n",
                "\n",
                "Indexing and selecting data are crucial skills for extracting relevant information from a DataFrame. Adjust the example code and considerations based on the specifics of your real-world scenarios and datasets.\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "8cff13e5-7a89-4e0a-b825-6e7453964a0d"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "\n",
                "### **`Descriptive Statistics in Pandas`**\n",
                "\n",
                "**Introduction:**\n",
                "Descriptive statistics aim to summarize and describe the main features of a dataset. Pandas provides various functions to compute descriptive statistics for each column in a DataFrame.\n",
                "\n",
                "**1. Mean:**\n",
                "   - **Definition:** The mean, also known as the average, is the sum of all values in a dataset divided by the number of observations.\n",
                "   - **Pandas Code:**\n",
                "     ```python\n",
                "     mean_values = df.mean()\n",
                "     ```\n",
                "   - **Interpretation:** The mean provides a measure of central tendency, indicating the typical value in a dataset.\n",
                "\n",
                "**2. Median:**\n",
                "   - **Definition:** The median is the middle value in a dataset when it is sorted in ascending order. It is less sensitive to extreme values than the mean.\n",
                "   - **Pandas Code:**\n",
                "     ```python\n",
                "     median_values = df.median()\n",
                "     ```\n",
                "   - **Interpretation:** The median gives insight into the central position of the data, especially in the presence of outliers.\n",
                "\n",
                "**3. Mode:**\n",
                "   - **Definition:** The mode represents the most frequently occurring value(s) in a dataset.\n",
                "   - **Pandas Code:**\n",
                "     ```python\n",
                "     mode_values = df.mode().iloc[0]\n",
                "     ```\n",
                "   - **Interpretation:** Identifying the mode helps in understanding the most common values in a dataset.\n",
                "\n",
                "**4. Standard Deviation:**\n",
                "   - **Definition:** The standard deviation measures the amount of variation or dispersion in a set of values. A higher standard deviation indicates greater variability.\n",
                "   - **Pandas Code:**\n",
                "     ```python\n",
                "     std_deviation = df.std()\n",
                "     ```\n",
                "   - **Interpretation:** Standard deviation is crucial for assessing the spread of values around the mean.\n",
                "\n",
                "**5. Variance:**\n",
                "   - **Definition:** Variance is the average of the squared differences from the mean. It is the square of the standard deviation.\n",
                "   - **Pandas Code:**\n",
                "     ```python\n",
                "     variance_values = df.var()\n",
                "     ```\n",
                "   - **Interpretation:** Variance provides another measure of data dispersion, useful in comparing the spread of different datasets.\n",
                "\n",
                "**6. Quantiles and Percentiles:**\n",
                "   - **Definition:** Quantiles divide a dataset into intervals with equal probabilities. Percentiles are specific quantiles expressed as percentages.\n",
                "   - **Pandas Code:**\n",
                "     ```python\n",
                "     quantiles = df.quantile([0.25, 0.5, 0.75])\n",
                "     ```\n",
                "   - **Interpretation:** Quantiles help in understanding the distribution and identifying central points in the data.\n",
                "\n",
                "**7. Interquartile Range (IQR):**\n",
                "   - **Definition:** IQR is the range between the first quartile (25th percentile) and the third quartile (75th percentile). It provides a measure of statistical dispersion.\n",
                "   - **Pandas Code:**\n",
                "     ```python\n",
                "     iqr_values = quantiles.loc[0.75] - quantiles.loc[0.25]\n",
                "     ```\n",
                "   - **Interpretation:** IQR is useful for identifying potential outliers and understanding the bulk of the data distribution.\n",
                "\n",
                "**8. Skewness:**\n",
                "   - **Definition:** Skewness measures the asymmetry of a distribution. Positive skewness indicates a right-skewed distribution, while negative skewness indicates a left-skewed distribution.\n",
                "   - **Pandas Code:**\n",
                "     ```python\n",
                "     skewness_values = df.skew()\n",
                "     ```\n",
                "   - **Interpretation:** Skewness provides insights into the shape of the distribution.\n",
                "\n",
                "**9. Kurtosis:**\n",
                "   - **Definition:** Kurtosis measures the sharpness of the peak (or tails) of a distribution. High kurtosis indicates a sharp peak and heavy tails.\n",
                "   - **Pandas Code:**\n",
                "     ```python\n",
                "     kurtosis_values = df.kurt()\n",
                "     ```\n",
                "   - **Interpretation:** Kurtosis helps in understanding the tails' thickness and the presence of outliers.\n",
                "\n",
                "**10. Correlation and Covariance:**\n",
                "   - **Definition:** Correlation measures the linear relationship between two variables, while covariance measures their joint variability.\n",
                "   - **Pandas Code:**\n",
                "     ```python\n",
                "     correlation_matrix = df.corr()\n",
                "     covariance_matrix = df.cov()\n",
                "     ```\n",
                "   - **Interpretation:** Correlation and covariance are crucial for understanding relationships between variables.\n",
                "\n",
                "**Conclusion:**\n",
                "Descriptive statistics in Pandas provide a comprehensive view of the distribution, relationships, and variability within a dataset. Understanding these measures is fundamental for data analysis and decision-making. The choice of which statistics to use depends on the nature of the data and the questions you want to answer."
            ],
            "metadata": {
                "azdata_cell_guid": "1452f9a1-4599-4d87-b269-6abf884c467e"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Example :\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "823bf2cf-4d7c-4493-909c-34b31d27d5f9"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# Creating a sample DataFrame\n",
                "data = {\n",
                "    'Age': [25, 30, 35, 22, 28],\n",
                "    'Salary': [50000, 60000, 75000, 48000, 55000],\n",
                "    'Experience': [3, 5, 8, 2, 4],\n",
                "}\n",
                "\n",
                "df = pd.DataFrame(data)\n",
                "\n",
                "# Mean, Median, and Mode\n",
                "mean_values = df.mean()\n",
                "median_values = df.median()\n",
                "mode_values = df.mode().iloc[0]\n",
                "\n",
                "# Measures of Dispersion\n",
                "std_deviation = df.std()\n",
                "variance_values = df.var()\n",
                "\n",
                "# Quantiles and Percentiles\n",
                "quantiles = df.quantile([0.25, 0.5, 0.75])\n",
                "iqr_values = quantiles.loc[0.75] - quantiles.loc[0.25]\n",
                "\n",
                "# Summary Statistics\n",
                "summary_stats = df.describe()\n",
                "\n",
                "# Skewness and Kurtosis\n",
                "skewness_values = df.skew()\n",
                "kurtosis_values = df.kurt()\n",
                "\n",
                "# Correlation and Covariance\n",
                "correlation_matrix = df.corr()\n",
                "covariance_matrix = df.cov()\n",
                "\n",
                "# Displaying the results\n",
                "print(\"Mean Values:\\n\", mean_values)\n",
                "print(\"\\nMedian Values:\\n\", median_values)\n",
                "print(\"\\nMode Values:\\n\", mode_values)\n",
                "print(\"\\nStandard Deviation:\\n\", std_deviation)\n",
                "print(\"\\nVariance Values:\\n\", variance_values)\n",
                "print(\"\\nQuantiles:\\n\", quantiles)\n",
                "print(\"\\nInterquartile Range (IQR):\\n\", iqr_values)\n",
                "print(\"\\nSummary Statistics:\\n\", summary_stats)\n",
                "print(\"\\nSkewness Values:\\n\", skewness_values)\n",
                "print(\"\\nKurtosis Values:\\n\", kurtosis_values)\n",
                "print(\"\\nCorrelation Matrix:\\n\", correlation_matrix)\n",
                "print(\"\\nCovariance Matrix:\\n\", covariance_matrix)\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "e0b9ce95-affd-4fea-b096-4ae3fb73bdd1",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Mean Values:\n Age              28.0\nSalary        57600.0\nExperience        4.4\ndtype: float64\n\nMedian Values:\n Age              28.0\nSalary        55000.0\nExperience        4.0\ndtype: float64\n\nMode Values:\n Age              22\nSalary        48000\nExperience        2\nName: 0, dtype: int64\n\nStandard Deviation:\n Age               4.949747\nSalary        10784.247772\nExperience        2.302173\ndtype: float64\n\nVariance Values:\n Age                  24.5\nSalary        116300000.0\nExperience            5.3\ndtype: float64\n\nQuantiles:\n        Age   Salary  Experience\n0.25  25.0  50000.0         3.0\n0.50  28.0  55000.0         4.0\n0.75  30.0  60000.0         5.0\n\nInterquartile Range (IQR):\n Age               5.0\nSalary        10000.0\nExperience        2.0\ndtype: float64\n\nSummary Statistics:\n              Age        Salary  Experience\ncount   5.000000      5.000000    5.000000\nmean   28.000000  57600.000000    4.400000\nstd     4.949747  10784.247772    2.302173\nmin    22.000000  48000.000000    2.000000\n25%    25.000000  50000.000000    3.000000\n50%    28.000000  55000.000000    4.000000\n75%    30.000000  60000.000000    5.000000\nmax    35.000000  75000.000000    8.000000\n\nSkewness Values:\n Age           0.371076\nSalary        1.309113\nExperience    1.032659\ndtype: float64\n\nKurtosis Values:\n Age          -0.099125\nSalary        1.571802\nExperience    1.128515\ndtype: float64\n\nCorrelation Matrix:\n                  Age    Salary  Experience\nAge         1.000000  0.969477    0.987260\nSalary      0.969477  1.000000    0.994876\nExperience  0.987260  0.994876    1.000000\n\nCovariance Matrix:\n                  Age       Salary  Experience\nAge            24.50      51750.0       11.25\nSalary      51750.00  116300000.0    24700.00\nExperience     11.25      24700.0        5.30\n"
                }
            ],
            "execution_count": 7
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Real-world Scenario:\n",
                "Consider a scenario where you have a dataset containing information about the performance of students in an educational institution. The dataset includes student IDs, exam scores in different subjects, attendance percentages, and participation in extracurricular activities. You want to extract descriptive statistics to gain insights into the students' academic performance."
            ],
            "metadata": {
                "azdata_cell_guid": "06732364-11da-472a-8de0-862d2dc3324a"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Sample student performance data\n",
                "student_data = {\n",
                "    'StudentID': [1, 2, 3, 4, 5],\n",
                "    'Math_Score': [85, 90, 78, 92, 88],\n",
                "    'English_Score': [75, 85, 80, 88, 92],\n",
                "    'Attendance_Percentage': [92, 95, 88, 97, 93],\n",
                "    'Extracurricular_Participation': [2, 3, 1, 4, 2],\n",
                "}\n",
                "\n",
                "# Creating a DataFrame from the student data\n",
                "df_students = pd.DataFrame(student_data)\n",
                "\n",
                "# Extracting Descriptive Statistics\n",
                "mean_scores = df_students.mean()\n",
                "median_scores = df_students.median()\n",
                "std_deviation_scores = df_students.std()\n",
                "attendance_summary = df_students['Attendance_Percentage'].describe()\n",
                "correlation_matrix = df_students.corr()\n",
                "\n",
                "# Displaying the Descriptive Statistics\n",
                "print(\"Mean Scores:\\n\", mean_scores)\n",
                "print(\"\\nMedian Scores:\\n\", median_scores)\n",
                "print(\"\\nStandard Deviation of Scores:\\n\", std_deviation_scores)\n",
                "print(\"\\nAttendance Summary:\\n\", attendance_summary)\n",
                "print(\"\\nCorrelation Matrix:\\n\", correlation_matrix)\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "d7a423ac-449b-43c5-b0eb-ecaadc52a272",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Mean Scores:\n StudentID                         3.0\nMath_Score                       86.6\nEnglish_Score                    84.0\nAttendance_Percentage            93.0\nExtracurricular_Participation     2.4\ndtype: float64\n\nMedian Scores:\n StudentID                         3.0\nMath_Score                       88.0\nEnglish_Score                    85.0\nAttendance_Percentage            93.0\nExtracurricular_Participation     2.0\ndtype: float64\n\nStandard Deviation of Scores:\n StudentID                        1.581139\nMath_Score                       5.458938\nEnglish_Score                    6.670832\nAttendance_Percentage            3.391165\nExtracurricular_Participation    1.140175\ndtype: float64\n\nAttendance Summary:\n count     5.000000\nmean     93.000000\nstd       3.391165\nmin      88.000000\n25%      92.000000\n50%      93.000000\n75%      95.000000\nmax      97.000000\nName: Attendance_Percentage, dtype: float64\n\nCorrelation Matrix:\n                                StudentID  Math_Score  English_Score  \\\nStudentID                       1.000000    0.231714       0.876984   \nMath_Score                      0.231714    1.000000       0.583540   \nEnglish_Score                   0.876984    0.583540       1.000000   \nAttendance_Percentage           0.186501    0.985839       0.519408   \nExtracurricular_Participation   0.138675    0.915788       0.427299   \n\n                               Attendance_Percentage  \\\nStudentID                                   0.186501   \nMath_Score                                  0.985839   \nEnglish_Score                               0.519408   \nAttendance_Percentage                       1.000000   \nExtracurricular_Participation               0.969864   \n\n                               Extracurricular_Participation  \nStudentID                                           0.138675  \nMath_Score                                          0.915788  \nEnglish_Score                                       0.427299  \nAttendance_Percentage                               0.969864  \nExtracurricular_Participation                       1.000000  \n"
                }
            ],
            "execution_count": 8
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Considerations or Peculiarities:\n",
                "\n",
                "- **Data Types:** Ensure that numeric columns are appropriately represented as either integers or floats. Some statistics, like correlation, require numeric data.\n",
                "\n",
                "- **Missing Values:** Descriptive statistics functions in Pandas automatically exclude missing values. Be aware of missing data and decide how to handle it.\n",
                "\n",
                "- **Correlation vs. Causation:** Correlation does not imply causation. When interpreting correlation values, be cautious about inferring causal relationships between variables.\n",
                "\n",
                "#### Common Mistakes:\n",
                "\n",
                "- **Misinterpreting Correlation:** A common mistake is assuming a strong correlation implies a cause-and-effect relationship. Always consider the context and potential confounding variables.\n",
                "\n",
                "- **Ignoring Missing Values:** Failing to address missing values before computing descriptive statistics can lead to inaccurate results. Use methods like `dropna()` or `fillna()` appropriately.\n",
                "\n",
                "- **Inconsistent Data Types:** Ensure that all numeric columns have consistent data types. Mixed data types in a numeric column may cause unexpected results.\n",
                "\n",
                "Descriptive statistics offer valuable insights into the central tendency, variability, and relationships within a dataset. Consider the specific characteristics of your data when selecting which statistics to compute and interpret the results in the context of your analysis.\n",
                ""
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "bc67c244-012f-409e-9a85-1474f26fefb5"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **`6. Data Cleaning and Handling Missing Data`**"
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "263e2485-d04e-46fc-a051-94e780134d02"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### **`Handling Missing Values`**\n",
                "\n",
                "#### Importance of Identifying and Handling Missing Values in a DataFrame:\n",
                "\n",
                "Missing values, represented as NaN (Not a Number) in Pandas, are a common occurrence in real-world datasets. Properly identifying and handling missing values is crucial for meaningful and accurate data analysis. Ignoring missing values can lead to biased results and incorrect interpretations. Here's why handling missing values is important:\n",
                "\n",
                "1. **Data Accuracy:** Missing values can distort summary statistics, such as mean and standard deviation, leading to inaccurate insights about the dataset.\n",
                "\n",
                "2. **Model Performance:** If missing values are not addressed, they can adversely impact machine learning models, causing biased predictions and reduced model performance.\n",
                "\n",
                "3. **Data Visualization:** Visualizations may not accurately represent the distribution of data when missing values are present, affecting the interpretation of results.\n",
                "\n",
                "4. **Statistical Analyses:** Many statistical analyses and tests assume complete data. Missing values can compromise the validity of statistical results and significance testing.\n",
                "\n",
                "#### Methods for Handling Missing Values:\n",
                "\n",
                "1. **Identifying Missing Values:**\n",
                "   - **`isna()` and `notna()`:**\n",
                "     ```python\n",
                "     # Check for missing values\n",
                "     df.isna()  # Returns a DataFrame of the same shape with True for missing values\n",
                "     df.notna()  # Returns the opposite of isna()\n",
                "     ```\n",
                "\n",
                "2. **Handling Missing Values:**\n",
                "   - **`fillna()`:**\n",
                "     ```python\n",
                "     # Fill missing values with a specified value or a calculated value\n",
                "     df.fillna(value)  # Fill with a constant value\n",
                "     df.fillna(df.mean())  # Fill with the mean of each column\n",
                "     ```\n",
                "\n",
                "   - **Dropping Missing Values:**\n",
                "     ```python\n",
                "     # Drop rows or columns containing missing values\n",
                "     df.dropna()  # Drop rows with any missing values\n",
                "     df.dropna(axis=1)  # Drop columns with any missing values\n",
                "     ```\n",
                "\n",
                "   - **Interpolation:**\n",
                "     ```python\n",
                "     # Interpolate missing values using various methods (linear, polynomial, etc.)\n",
                "     df.interpolate()\n",
                "     ```\n",
                "\n",
                "#### Considerations and Best Practices:\n",
                "\n",
                "- **Context Matters:** The method chosen to handle missing values depends on the nature of the data and the reason for missingness. Consider the context before applying a specific strategy.\n",
                "\n",
                "- **Impact on Analysis:** Understand how the chosen method might impact your analysis. For example, filling missing values with the mean could introduce bias if missingness is not random.\n",
                "\n",
                "- **Visualization:** Visualize the distribution of missing values using tools like heatmaps to better understand patterns of missingness.\n",
                "\n",
                "- **Documentation:** Clearly document the chosen strategy for handling missing values in your analysis to ensure transparency and reproducibility.\n",
                "\n",
                "#### Conclusion:\n",
                "\n",
                "Properly handling missing values is a critical step in the data cleaning process. It ensures the integrity of analyses and models, leading to more reliable and accurate results. Familiarizing yourself with Pandas methods like `isna()`, `notna()`, and `fillna()` empowers you to make informed decisions when dealing with missing data in your DataFrame."
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "7f35f020-e667-4e20-9d45-9e9a2c73879f"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Example:\n",
                "Consider a scenario where we have a DataFrame containing information about students' exam scores in different subjects. The dataset has missing values that need to be handled, and we'll demonstrate the use of `isna()`, `notna()`, and `fillna()` to address these missing values."
            ],
            "metadata": {
                "azdata_cell_guid": "bc6692dd-28ab-45d8-8053-e50bc49d4ff5"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# Sample student exam data with missing values\n",
                "exam_data = {\n",
                "    'StudentID': [1, 2, 3, 4, 5],\n",
                "    'Math_Score': [85, np.nan, 78, 92, 88],\n",
                "    'English_Score': [75, 85, np.nan, 88, 92],\n",
                "    'Physics_Score': [90, 78, 85, np.nan, 94],\n",
                "    'Chemistry_Score': [82, 88, 90, 76, np.nan],\n",
                "}\n",
                "\n",
                "# Creating a DataFrame from the exam data\n",
                "df_exams = pd.DataFrame(exam_data)\n",
                "\n",
                "# Displaying the original DataFrame\n",
                "print(\"Original DataFrame:\")\n",
                "print(df_exams)\n",
                "\n",
                "# Identifying missing values\n",
                "missing_values = df_exams.isna()\n",
                "print(\"\\nMissing Values:\")\n",
                "print(missing_values)\n",
                "\n",
                "# Filling missing values with the mean of each column\n",
                "mean_filled_df = df_exams.fillna(df_exams.mean())\n",
                "\n",
                "# Displaying the DataFrame after handling missing values\n",
                "print(\"\\nDataFrame after Filling Missing Values with Mean:\")\n",
                "print(mean_filled_df)\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "f2407cdd-6b01-4227-ae34-229bdafb3946",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Original DataFrame:\n   StudentID  Math_Score  English_Score  Physics_Score  Chemistry_Score\n0          1        85.0           75.0           90.0             82.0\n1          2         NaN           85.0           78.0             88.0\n2          3        78.0            NaN           85.0             90.0\n3          4        92.0           88.0            NaN             76.0\n4          5        88.0           92.0           94.0              NaN\n\nMissing Values:\n   StudentID  Math_Score  English_Score  Physics_Score  Chemistry_Score\n0      False       False          False          False            False\n1      False        True          False          False            False\n2      False       False           True          False            False\n3      False       False          False           True            False\n4      False       False          False          False             True\n\nDataFrame after Filling Missing Values with Mean:\n   StudentID  Math_Score  English_Score  Physics_Score  Chemistry_Score\n0          1       85.00           75.0          90.00             82.0\n1          2       85.75           85.0          78.00             88.0\n2          3       78.00           85.0          85.00             90.0\n3          4       92.00           88.0          86.75             76.0\n4          5       88.00           92.0          94.00             84.0\n"
                }
            ],
            "execution_count": 9
        },
        {
            "cell_type": "markdown",
            "source": [
                "In the above example:\n",
                "\n",
                "1. **Identifying Missing Values:**\n",
                "   - We use `isna()` to create a DataFrame of the same shape as the original, with `True` values where missing values are present.\n",
                "\n",
                "2. **Handling Missing Values:**\n",
                "   - We use `fillna()` to fill missing values with the mean of each column.\n",
                "\n",
                "3. **Result:**\n",
                "   - The final DataFrame (`mean_filled_df`) has missing values filled with the mean of each respective column.\n",
                "\n",
                "This example showcases the importance of identifying and handling missing values and demonstrates a practical approach using Pandas methods. Adjust the code based on your specific dataset and requirements."
            ],
            "metadata": {
                "azdata_cell_guid": "313410ac-ddd0-4aff-9aca-41d484b41586"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Real-world Scenario:\n",
                "Consider a scenario where you have a dataset containing information about customer orders in an e-commerce platform. The dataset includes order IDs, product names, quantities, prices, and shipping dates. Due to various reasons such as system glitches or customer actions, some data is missing. Let's explore how to identify and handle missing values in this context."
            ],
            "metadata": {
                "azdata_cell_guid": "39de8df7-97c5-41d3-be8e-1a350b51c76e"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# Sample e-commerce order data with missing values\n",
                "order_data = {\n",
                "    'OrderID': [101, 102, np.nan, 104, 105],\n",
                "    'Product': ['Laptop', 'Smartphone', 'Tablet', np.nan, 'Camera'],\n",
                "    'Quantity': [2, 1, np.nan, 2, 1],\n",
                "    'Price': [1200, 800, 300, np.nan, 700],\n",
                "    'Shipping_Date': ['2022-01-01', '2022-01-02', np.nan, '2022-01-03', '2022-01-03'],\n",
                "}\n",
                "\n",
                "# Creating a DataFrame from the order data\n",
                "df_orders = pd.DataFrame(order_data)\n",
                "\n",
                "# Displaying the original DataFrame\n",
                "print(\"Original Order DataFrame:\")\n",
                "print(df_orders)\n",
                "\n",
                "# Identifying missing values\n",
                "missing_values = df_orders.isna()\n",
                "print(\"\\nMissing Values:\")\n",
                "print(missing_values)\n",
                "\n",
                "# Handling missing values by dropping rows with missing OrderID and filling Price and Shipping_Date\n",
                "df_orders_cleaned = df_orders.dropna(subset=['OrderID']).fillna({'Price': df_orders['Price'].mean(), 'Shipping_Date': '2022-01-01'})\n",
                "\n",
                "# Displaying the DataFrame after handling missing values\n",
                "print(\"\\nDataFrame after Handling Missing Values:\")\n",
                "print(df_orders_cleaned)\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "79a1e486-5e98-4fae-9c91-5c4d02f5235d",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Original Order DataFrame:\n   OrderID     Product  Quantity   Price Shipping_Date\n0    101.0      Laptop       2.0  1200.0    2022-01-01\n1    102.0  Smartphone       1.0   800.0    2022-01-02\n2      NaN      Tablet       NaN   300.0           NaN\n3    104.0         NaN       2.0     NaN    2022-01-03\n4    105.0      Camera       1.0   700.0    2022-01-03\n\nMissing Values:\n   OrderID  Product  Quantity  Price  Shipping_Date\n0    False    False     False  False          False\n1    False    False     False  False          False\n2     True    False      True  False           True\n3    False     True     False   True          False\n4    False    False     False  False          False\n\nDataFrame after Handling Missing Values:\n   OrderID     Product  Quantity   Price Shipping_Date\n0    101.0      Laptop       2.0  1200.0    2022-01-01\n1    102.0  Smartphone       1.0   800.0    2022-01-02\n3    104.0         NaN       2.0   750.0    2022-01-03\n4    105.0      Camera       1.0   700.0    2022-01-03\n"
                }
            ],
            "execution_count": 10
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Considerations or Peculiarities:\n",
                "\n",
                "- **Reasons for Missingness:**\n",
                "  - Understand the reasons for missing values. In this example, missing OrderID might be due to a system error, missing Product might be due to a new product without details, and missing Price and Shipping_Date might be due to incomplete data.\n",
                "\n",
                "- **Impact on Analysis:**\n",
                "  - Consider how missing values might impact your analysis. Dropping rows or filling missing values should align with the analysis goals.\n",
                "\n",
                "- **Domain Knowledge:**\n",
                "  - Domain knowledge is crucial for deciding how to handle missing values appropriately. For example, filling a missing Price with the mean might not be suitable if prices vary significantly.\n",
                "\n",
                "#### Common Mistakes:\n",
                "\n",
                "- **Ignoring Missing Values:**\n",
                "  - Ignoring missing values without assessing their impact on analyses can lead to biased results.\n",
                "\n",
                "- **Unintended Dropping:**\n",
                "  - Unintentionally dropping rows or columns without considering the reasons for missingness may result in data loss and incomplete analyses.\n",
                "\n",
                "- **Inconsistent Handling:**\n",
                "  - Inconsistently handling missing values across different columns or datasets can introduce inconsistencies in your analysis.\n",
                "\n",
                "Handling missing values requires careful consideration and should be aligned with the overall data analysis goals. It's essential to understand the dataset's context and choose appropriate strategies based on the nature of the missing data.\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "93090d63-42d7-4207-a9b0-9910ecbfa694"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### **`Dropping or Filling Missing Values`**\n",
                "\n",
                "#### Decision-Making Process:\n",
                "\n",
                "1. **Dropping Missing Values:**\n",
                "   - **Context:** Dropping missing values is suitable when the missingness is random, and removing incomplete records doesn't introduce bias or impact the analysis significantly. It's a pragmatic approach when the missing data is negligible compared to the dataset size.\n",
                "\n",
                "   - **Example:**\n",
                "     ```python\n",
                "     # Drop rows with any missing values\n",
                "     df_dropped = df.dropna()\n",
                "     ```\n",
                "\n",
                "2. **Filling Missing Values:**\n",
                "   - **Context:** Filling missing values is appropriate when retaining the incomplete records is crucial, and a reasonable estimation can be made for the missing values. This is common when dealing with time-series data, where continuity matters.\n",
                "\n",
                "   - **Example:**\n",
                "     ```python\n",
                "     # Fill missing values in 'column_name' with a constant value\n",
                "     df_filled_constant = df.fillna(value=0)\n",
                "     ```\n",
                "\n",
                "   - **Example:**\n",
                "     ```python\n",
                "     # Fill missing values with the mean of each column\n",
                "     df_filled_mean = df.fillna(df.mean())\n",
                "     ```\n",
                "\n",
                "   - **Example:**\n",
                "     ```python\n",
                "     # Forward fill missing values in a DataFrame\n",
                "     df_forward_filled = df.ffill()\n",
                "     ```\n",
                "\n",
                "   - **Example:**\n",
                "     ```python\n",
                "     # Backward fill missing values in a DataFrame\n",
                "     df_backward_filled = df.bfill()\n",
                "     ```\n",
                "\n",
                "   - **Example:**\n",
                "     ```python\n",
                "     # Interpolate missing values using linear interpolation\n",
                "     df_interpolated_linear = df.interpolate(method='linear')\n",
                "     ```\n",
                "\n",
                "#### Considerations:\n",
                "\n",
                "- **Data Nature:**\n",
                "  - Consider the nature of the data. For time-series data, forward or backward filling might be suitable, while for numeric data, mean or interpolation might be appropriate.\n",
                "\n",
                "- **Impact on Analysis:**\n",
                "  - Evaluate how the chosen method for handling missing values might impact subsequent analyses. Ensure that the imputation method aligns with the overall analysis goals.\n",
                "\n",
                "- **Domain Knowledge:**\n",
                "  - Leverage domain knowledge to make informed decisions. Some missing values may be inherently unfillable due to the nature of the data.\n",
                "\n",
                "#### Conclusion:\n",
                "\n",
                "The decision between dropping or filling missing values depends on the specific characteristics of the data and the analysis goals. Dropping values is a straightforward approach but may lead to data loss. Filling values is a more nuanced process, requiring careful consideration of the data's nature and the impact on downstream analyses. Experiment with different strategies and choose the one that best fits the context of your dataset."
            ],
            "metadata": {
                "azdata_cell_guid": "7aaec997-7d35-4bd0-9a0d-7fd85a45db16"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Example:\n",
                "\n",
                "Let's consider a scenario where we have a DataFrame representing monthly sales data for a product. The dataset has missing values in the 'Sales' column, and we need to decide whether to drop or fill those missing values based on the context."
            ],
            "metadata": {
                "azdata_cell_guid": "9562c6ed-5366-4e62-9417-958a3745ad27"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# Sample monthly sales data with missing values\n",
                "sales_data = {\n",
                "    'Month': ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun'],\n",
                "    'Sales': [100, 120, np.nan, 150, np.nan, 180],\n",
                "}\n",
                "\n",
                "# Creating a DataFrame from the sales data\n",
                "df_sales = pd.DataFrame(sales_data)\n",
                "\n",
                "# Displaying the original DataFrame\n",
                "print(\"Original Sales DataFrame:\")\n",
                "print(df_sales)\n",
                "\n",
                "# Decision 1: Dropping Missing Values\n",
                "df_dropped = df_sales.dropna()\n",
                "\n",
                "# Displaying the DataFrame after dropping missing values\n",
                "print(\"\\nDataFrame after Dropping Missing Values:\")\n",
                "print(df_dropped)\n",
                "\n",
                "# Decision 2: Filling Missing Values with Forward Fill\n",
                "df_filled_forward = df_sales.ffill()\n",
                "\n",
                "# Displaying the DataFrame after forward filling missing values\n",
                "print(\"\\nDataFrame after Forward Filling Missing Values:\")\n",
                "print(df_filled_forward)\n",
                "\n",
                "# Decision 3: Filling Missing Values with Mean\n",
                "df_filled_mean = df_sales.fillna(df_sales['Sales'].mean())\n",
                "\n",
                "# Displaying the DataFrame after filling missing values with mean\n",
                "print(\"\\nDataFrame after Filling Missing Values with Mean:\")\n",
                "print(df_filled_mean)\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "664209a4-e5a6-4ba0-b0ff-ccaf4e068478",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Original Sales DataFrame:\n  Month  Sales\n0   Jan  100.0\n1   Feb  120.0\n2   Mar    NaN\n3   Apr  150.0\n4   May    NaN\n5   Jun  180.0\n\nDataFrame after Dropping Missing Values:\n  Month  Sales\n0   Jan  100.0\n1   Feb  120.0\n3   Apr  150.0\n5   Jun  180.0\n\nDataFrame after Forward Filling Missing Values:\n  Month  Sales\n0   Jan  100.0\n1   Feb  120.0\n2   Mar  120.0\n3   Apr  150.0\n4   May  150.0\n5   Jun  180.0\n\nDataFrame after Filling Missing Values with Mean:\n  Month  Sales\n0   Jan  100.0\n1   Feb  120.0\n2   Mar  137.5\n3   Apr  150.0\n4   May  137.5\n5   Jun  180.0\n"
                }
            ],
            "execution_count": 11
        },
        {
            "cell_type": "markdown",
            "source": [
                "In this example:\n",
                "\n",
                "1. **Dropping Missing Values:**\n",
                "   - We use `dropna()` to remove rows with any missing values. This might be suitable if missing values are limited and their removal doesn't significantly affect the analysis.\n",
                "\n",
                "2. **Filling Missing Values with Forward Fill:**\n",
                "   - We use `ffill()` to fill missing values with the previous month's sales. This approach is reasonable when the missing values follow a pattern and can be reasonably estimated using existing data.\n",
                "\n",
                "3. **Filling Missing Values with Mean:**\n",
                "   - We use `fillna()` with the mean of the 'Sales' column to impute missing values. This approach is suitable when we want to retain all rows and fill missing values with a representative value.\n",
                "\n",
                "Adjust the code based on the specific characteristics of your dataset and the analysis goals. Choosing between dropping or filling missing values should be driven by the dataset's context and the impact on subsequent analyses."
            ],
            "metadata": {
                "azdata_cell_guid": "45516b54-4e93-4eb9-9bb6-7e0172cfdd3b"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Real-world Scenario:\n",
                "\n",
                "Imagine you are managing a dataset that tracks monthly sales data for a retail business. The dataset includes information such as the month, product category, sales quantity, and revenue. However, due to occasional reporting errors or data collection issues, there are missing values in the dataset. Let's explore how to handle these missing values using Pandas."
            ],
            "metadata": {
                "azdata_cell_guid": "285c7b1a-2b52-480c-8a78-91aff87490ec"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# Sample monthly sales data with missing values\n",
                "sales_data = {\n",
                "    'Month': ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun'],\n",
                "    'Category': ['Electronics', 'Clothing', np.nan, 'Electronics', np.nan, 'Clothing'],\n",
                "    'Sales_Quantity': [120, 150, np.nan, 200, np.nan, 180],\n",
                "    'Revenue': [12000, np.nan, 18000, np.nan, 25000, 22000],\n",
                "}\n",
                "\n",
                "# Creating a DataFrame from the sales data\n",
                "df_sales = pd.DataFrame(sales_data)\n",
                "\n",
                "# Displaying the original DataFrame\n",
                "print(\"Original Monthly Sales DataFrame:\")\n",
                "print(df_sales)\n",
                "\n",
                "# Handling Missing Values:\n",
                "# Decision 1: Dropping rows with any missing values\n",
                "df_dropped = df_sales.dropna()\n",
                "\n",
                "# Decision 2: Filling missing values with mean for numerical columns\n",
                "df_filled_mean = df_sales.fillna({'Sales_Quantity': df_sales['Sales_Quantity'].mean(), 'Revenue': df_sales['Revenue'].mean()})\n",
                "\n",
                "# Displaying the DataFrames after handling missing values\n",
                "print(\"\\nDataFrame after Dropping Missing Values:\")\n",
                "print(df_dropped)\n",
                "\n",
                "print(\"\\nDataFrame after Filling Missing Values with Mean:\")\n",
                "print(df_filled_mean)\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "6c2e596f-1e7c-40b6-bcee-49369a670058",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Original Monthly Sales DataFrame:\n  Month     Category  Sales_Quantity  Revenue\n0   Jan  Electronics           120.0  12000.0\n1   Feb     Clothing           150.0      NaN\n2   Mar          NaN             NaN  18000.0\n3   Apr  Electronics           200.0      NaN\n4   May          NaN             NaN  25000.0\n5   Jun     Clothing           180.0  22000.0\n\nDataFrame after Dropping Missing Values:\n  Month     Category  Sales_Quantity  Revenue\n0   Jan  Electronics           120.0  12000.0\n5   Jun     Clothing           180.0  22000.0\n\nDataFrame after Filling Missing Values with Mean:\n  Month     Category  Sales_Quantity  Revenue\n0   Jan  Electronics           120.0  12000.0\n1   Feb     Clothing           150.0  19250.0\n2   Mar          NaN           162.5  18000.0\n3   Apr  Electronics           200.0  19250.0\n4   May          NaN           162.5  25000.0\n5   Jun     Clothing           180.0  22000.0\n"
                }
            ],
            "execution_count": 12
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Considerations or Peculiarities:\n",
                "\n",
                "- **Imputation Strategy:**\n",
                "  - Choosing between dropping and filling depends on the impact on analysis. Dropping may lead to loss of important information, while filling may introduce bias if not done carefully.\n",
                "\n",
                "- **Context of Data:**\n",
                "  - Understand the context of your data. For example, filling missing revenue values with the mean might be reasonable, but for product categories, it may not make sense.\n",
                "\n",
                "- **Column-specific Strategies:**\n",
                "  - Different columns may require different strategies. For numeric columns, mean or median filling could be appropriate, while for categorical columns, forward fill or mode filling might be more suitable.\n",
                "\n",
                "#### Common Mistakes:\n",
                "\n",
                "- **Unintended Data Loss:**\n",
                "  - Developers might drop rows without considering the impact on the dataset's integrity. This can lead to unintended data loss, especially if the missing values are not randomly distributed.\n",
                "\n",
                "- **Inconsistent Imputation:**\n",
                "  - Filling missing values inconsistently across columns or datasets can introduce inconsistencies in the dataset.\n",
                "\n",
                "- **Overlooking Context:**\n",
                "  - Filling missing values without understanding the context of the data and the reasons for missingness may lead to inaccurate imputations.\n",
                "\n",
                "Handling missing values is a critical aspect of data preprocessing. It requires thoughtful consideration of the dataset's context, the nature of missingness, and the impact on downstream analyses. Developers should choose strategies that align with the goals of their analysis and avoid common pitfalls that can compromise data quality."
            ],
            "metadata": {
                "azdata_cell_guid": "026fd330-19c0-4239-92c0-d1406f9d4004"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### **`Removing Duplicates in a DataFrame`**\n",
                "\n",
                "#### Significance of Identifying and Removing Duplicate Rows:\n",
                "\n",
                "**1. Data Accuracy:**\n",
                "   - Duplicate rows can distort analyses by inflating counts, averages, or other summary statistics. Removing duplicates ensures the accuracy of calculated metrics.\n",
                "\n",
                "**2. Consistent Results:**\n",
                "   - Duplicates can lead to inconsistencies in results, especially in scenarios where aggregated data or distinct counts are essential.\n",
                "\n",
                "**3. Efficient Memory Usage:**\n",
                "   - Datasets with duplicate rows consume more memory. Eliminating duplicates optimizes memory usage and enhances computational efficiency.\n",
                "\n",
                "**4. Meaningful Insights:**\n",
                "   - Duplicate rows may not contribute meaningful insights but can skew results. Removing them ensures a cleaner dataset for analysis.\n",
                "\n",
                "#### Examples of Removing Duplicates:\n",
                "\n",
                "**1. Identifying Duplicate Rows:**\n",
                "```python\n",
                "# Check for duplicate rows based on all columns\n",
                "duplicates = df.duplicated()\n",
                "\n",
                "# Check for duplicate rows based on specific columns\n",
                "duplicates_specific_columns = df.duplicated(subset=['Column1', 'Column2'])\n",
                "```\n",
                "\n",
                "**2. Removing Duplicate Rows:**\n",
                "```python\n",
                "# Remove all duplicate rows, keeping the first occurrence\n",
                "df_no_duplicates = df.drop_duplicates()\n",
                "\n",
                "# Remove duplicate rows based on specific columns, keeping the first occurrence\n",
                "df_no_duplicates_specific_columns = df.drop_duplicates(subset=['Column1', 'Column2'])\n",
                "```\n",
                "\n",
                "#### Considerations:\n",
                "\n",
                "- **Column Selection:**\n",
                "  - Consider the columns relevant to duplicate identification. In some cases, duplicates may only be duplicates when considering specific columns.\n",
                "\n",
                "- **Order Matters:**\n",
                "  - `drop_duplicates()` retains the first occurrence and removes subsequent duplicates. Ensure the order aligns with your analysis goals.\n",
                "\n",
                "- **In-Place vs. New DataFrame:**\n",
                "  - Decide whether to modify the existing DataFrame in-place or create a new one. Choose based on the need to retain the original data.\n",
                "\n",
                "#### Common Mistakes:\n",
                "\n",
                "- **Ignoring Specific Columns:**\n",
                "  - Failing to specify columns during duplicate checking can result in unintended removal of rows that might be duplicates only in certain columns.\n",
                "\n",
                "- **Overlooking Order:**\n",
                "  - If retaining the first occurrence is essential, ensure that the DataFrame is sorted appropriately before using `drop_duplicates()`.\n",
                "\n",
                "- **Inconsistent Usage:**\n",
                "  - Inconsistently applying duplicate removal across different datasets or analyses can lead to inconsistent results.\n",
                "\n",
                "#### Conclusion:\n",
                "\n",
                "Identifying and removing duplicate rows is a crucial step in data cleaning and preprocessing. It enhances the accuracy of analyses, ensures meaningful insights, and optimizes memory usage. Developers should carefully consider the columns involved, the order of removal, and whether to modify the DataFrame in-place when handling duplicates."
            ],
            "metadata": {
                "azdata_cell_guid": "837e4b0f-9901-46c0-872a-1c2570de664b"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Example:\n",
                "\n",
                "Let's consider a scenario where you have a DataFrame containing data on customer orders, and due to data entry errors or system glitches, there are duplicate entries. We'll explore how to identify and remove these duplicate rows using Pandas."
            ],
            "metadata": {
                "azdata_cell_guid": "acb21bfa-ae4b-4d3e-927f-433680fa7259"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Sample order data with duplicate entries\n",
                "order_data = {\n",
                "    'OrderID': [101, 102, 101, 103, 104, 102],\n",
                "    'Product': ['Laptop', 'Smartphone', 'Laptop', 'Tablet', 'Camera', 'Smartphone'],\n",
                "    'Quantity': [2, 1, 1, 3, 1, 1],\n",
                "    'Total_Price': [1200, 800, 1200, 450, 700, 800],\n",
                "}\n",
                "\n",
                "# Creating a DataFrame from the order data\n",
                "df_orders = pd.DataFrame(order_data)\n",
                "\n",
                "# Displaying the original DataFrame\n",
                "print(\"Original Order DataFrame:\")\n",
                "print(df_orders)\n",
                "\n",
                "# Identifying Duplicate Rows\n",
                "duplicates = df_orders.duplicated()\n",
                "\n",
                "# Displaying duplicate rows\n",
                "print(\"\\nDuplicate Rows:\")\n",
                "print(df_orders[duplicates])\n",
                "\n",
                "# Removing Duplicate Rows\n",
                "df_no_duplicates = df_orders.drop_duplicates()\n",
                "\n",
                "# Displaying the DataFrame after removing duplicates\n",
                "print(\"\\nDataFrame after Removing Duplicates:\")\n",
                "print(df_no_duplicates)\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "1c28f6cf-708a-452e-bd65-3453c9d69a37",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Original Order DataFrame:\n   OrderID     Product  Quantity  Total_Price\n0      101      Laptop         2         1200\n1      102  Smartphone         1          800\n2      101      Laptop         1         1200\n3      103      Tablet         3          450\n4      104      Camera         1          700\n5      102  Smartphone         1          800\n\nDuplicate Rows:\n   OrderID     Product  Quantity  Total_Price\n5      102  Smartphone         1          800\n\nDataFrame after Removing Duplicates:\n   OrderID     Product  Quantity  Total_Price\n0      101      Laptop         2         1200\n1      102  Smartphone         1          800\n2      101      Laptop         1         1200\n3      103      Tablet         3          450\n4      104      Camera         1          700\n"
                }
            ],
            "execution_count": 13
        },
        {
            "cell_type": "markdown",
            "source": [
                "In this example:\n",
                "\n",
                "1. **Identifying Duplicate Rows:**\n",
                "   - We use `duplicated()` to identify duplicate rows based on all columns. The result is a boolean series indicating which rows are duplicates.\n",
                "\n",
                "2. **Displaying Duplicate Rows:**\n",
                "   - We use boolean indexing to display the rows that are identified as duplicates.\n",
                "\n",
                "3. **Removing Duplicate Rows:**\n",
                "   - We use `drop_duplicates()` to remove duplicate rows, keeping the first occurrence of each unique row.\n",
                "\n",
                "4. **Displaying Result:**\n",
                "   - We display the DataFrame after removing duplicates to see the cleaned dataset.\n",
                "\n",
                "Adjust the code based on your specific dataset and analysis goals. Understanding the significance of removing duplicates and applying these methods ensures a cleaner and more reliable dataset for further analysis."
            ],
            "metadata": {
                "azdata_cell_guid": "dc72d46b-a3e1-4ecd-a7a3-ba6245d3af51"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Considerations or Peculiarities:\n",
                "\n",
                "- **Column Selection:**\n",
                "  - Consider which columns should be considered for identifying duplicates. In some cases, duplicates may only be duplicates when considering specific columns.\n",
                "\n",
                "- **Impact on Analysis:**\n",
                "  - Consider how duplicate rows might impact subsequent analyses. Retaining duplicates might skew results, while removing them ensures a cleaner dataset.\n",
                "\n",
                "#### Common Mistakes:\n",
                "\n",
                "- **Incomplete Duplicate Identification:**\n",
                "  - Not considering all relevant columns during duplicate identification might result in incomplete removal of duplicates.\n",
                "\n",
                "- **Ignoring Context:**\n",
                "  - Failing to understand the context of the data might lead to unintended removal of rows that may be legitimate duplicates.\n",
                "\n",
                "- **Overlooking Order:**\n",
                "  - Forgetting to sort the DataFrame appropriately before using `drop_duplicates()` may lead to unexpected results if order matters.\n",
                "\n",
                "Handling duplicate rows is essential for maintaining data accuracy and ensuring meaningful analyses. Developers should carefully choose columns for duplicate identification, understand the impact of duplicates on analysis, and avoid common mistakes that could compromise data integrity."
            ],
            "metadata": {
                "azdata_cell_guid": "b9cd9f63-28e2-4f01-bad7-195202773652"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **`Hands On Experience:`**\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "ab6c04c5-9850-4c24-b3e8-e36e50a7a342"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Question 1: Creating a DataFrame from Lists and Basic Operations\n",
                "\n",
                "#### Scenario:\n",
                "You have information about monthly sales for a retail store. Each list contains data for a different month.\n",
                "\n",
                "```python\n",
                "# Data for three months\n",
                "months = ['Jan', 'Feb', 'Mar']\n",
                "sales = [1200, 1500, 1800]\n",
                "expenses = [800, 900, 1000]\n",
                "\n",
                "# Question:\n",
                "# Create a DataFrame named 'df_sales' from these lists, and display the DataFrame.\n",
                "# Calculate the profit for each month (Profit = Sales - Expenses).\n",
                "# Display the DataFrame after adding the 'Profit' column.\n",
                "```"
            ],
            "metadata": {
                "azdata_cell_guid": "e70630dc-03da-4fa8-b8d5-854d6ab84bc8"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Data for three months\n",
                "months = ['Jan', 'Feb', 'Mar']\n",
                "sales = [1200, 1500, 1800]\n",
                "expenses = [800, 900, 1000]\n",
                "\n",
                "import pandas as pd\n",
                "\n",
                "# Creating a DataFrame from Lists\n",
                "df_sales = pd.DataFrame({'Month': months, 'Sales': sales, 'Expenses': expenses})\n",
                "\n",
                "# Calculating Profit\n",
                "df_sales['Profit'] = df_sales['Sales'] - df_sales['Expenses']\n",
                "\n",
                "# Displaying the DataFrame\n",
                "print(\"DataFrame after Creating and Calculating Profit:\")\n",
                "print(df_sales)"
            ],
            "metadata": {
                "azdata_cell_guid": "9b9a88f4-4c9f-45de-90cd-58780b5f6917",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "DataFrame after Creating and Calculating Profit:\n  Month  Sales  Expenses  Profit\n0   Jan   1200       800     400\n1   Feb   1500       900     600\n2   Mar   1800      1000     800\n"
                }
            ],
            "execution_count": 14
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Question 2: Reading Data from CSV and Descriptive Statistics\n",
                "\n",
                "#### Scenario:\n",
                "\n",
                "Let's assume you have a CSV file named 'sales_data.csv' with the following structure:\n",
                "\n",
                "```csv\n",
                "Product,Quantity,Revenue\n",
                "Laptop,10,12000\n",
                "Smartphone,5,8000\n",
                "Tablet,,4500\n",
                "Camera,3,\n",
                "```\n",
                "\n",
                "You have a CSV file named 'sales_data.csv' containing information about product sales. Read the data into a DataFrame and perform descriptive statistics.\n",
                "\n",
                "```python\n",
                "# Question:\n",
                "# Read 'sales_data.csv' into a DataFrame named 'df_sales'.\n",
                "# Display the first 5 rows of the DataFrame.\n",
                "# Calculate basic descriptive statistics for the 'Quantity' column.\n",
                "```\n",
                "\n",
                ""
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "e465e295-2480-4a40-b09e-72200205114b"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Reading Data from CSV\n",
                "df_sales = pd.read_csv('sales_data.csv')\n",
                "\n",
                "# Displaying the first 5 rows\n",
                "print(\"First 5 Rows of df_sales:\")\n",
                "print(df_sales.head())\n",
                "\n",
                "# Descriptive Statistics for 'Quantity'\n",
                "quantity_stats = df_sales['Quantity'].describe()\n",
                "print(\"\\nDescriptive Statistics for 'Quantity':\")\n",
                "print(quantity_stats)"
            ],
            "metadata": {
                "azdata_cell_guid": "82a87d30-a038-4b31-bc39-095700296fd1",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "First 5 Rows of df_sales:\n      Product  Quantity  Revenue\n0      Laptop      10.0  12000.0\n1  Smartphone       5.0   8000.0\n2      Tablet       NaN   4500.0\n3      Camera       3.0      NaN\n\nDescriptive Statistics for 'Quantity':\ncount     3.000000\nmean      6.000000\nstd       3.605551\nmin       3.000000\n25%       4.000000\n50%       5.000000\n75%       7.500000\nmax      10.000000\nName: Quantity, dtype: float64\n"
                }
            ],
            "execution_count": 16
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Question 3: Handling Missing Values and Filling with Mean\n",
                "\n",
                "#### Scenario:\n",
                "Your DataFrame has missing values in the 'Revenue' column. Handle the missing values by filling them with the mean.\n",
                "\n",
                "```python\n",
                "# Question:\n",
                "# Handle missing values in the 'Revenue' column by filling them with the mean.\n",
                "# Display the DataFrame after handling missing values.\n",
                "```"
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "aea23b45-6f91-4710-8f6f-b81b3288c27c"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Handling Missing Values in 'Revenue'\n",
                "df_sales['Revenue'].fillna(df_sales['Revenue'].mean(), inplace=True)\n",
                "\n",
                "# Displaying the DataFrame after Handling Missing Values\n",
                "print(\"DataFrame after Handling Missing Values in 'Revenue':\")\n",
                "print(df_sales)"
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "690e01e6-b8d7-48bd-b8ac-16b8c42fdd9d"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "DataFrame after Handling Missing Values in 'Revenue':\n      Product  Quantity       Revenue\n0      Laptop      10.0  12000.000000\n1  Smartphone       5.0   8000.000000\n2      Tablet       NaN   4500.000000\n3      Camera       3.0   8166.666667\n"
                }
            ],
            "execution_count": 17
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Question 4: Removing Duplicates\n",
                "\n",
                "#### Scenario:\n",
                "Your DataFrame 'df_orders' contains duplicate entries for customer orders. Remove the duplicates based on all columns.\n",
                "\n",
                "```python\n",
                "# Question:\n",
                "# Identify and remove duplicate rows from 'df_orders'.\n",
                "# Display the DataFrame after removing duplicates.\n",
                "```"
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "d1aaff2f-7b85-437e-9f2e-5284ed710fb7"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Identifying Duplicate Rows\n",
                "duplicates = df_orders.duplicated()\n",
                "\n",
                "# Removing Duplicate Rows\n",
                "df_orders_no_duplicates = df_orders.drop_duplicates()\n",
                "\n",
                "# Displaying the DataFrame after Removing Duplicates\n",
                "print(\"DataFrame after Removing Duplicates:\")\n",
                "print(df_orders_no_duplicates)"
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "4f6450fc-56f3-4daf-93ae-6b527df3fd4e"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "DataFrame after Removing Duplicates:\n   OrderID     Product  Quantity  Total_Price\n0      101      Laptop         2         1200\n1      102  Smartphone         1          800\n2      101      Laptop         1         1200\n3      103      Tablet         3          450\n4      104      Camera         1          700\n"
                }
            ],
            "execution_count": 18
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Question 5: Conditional Indexing and Filtering\n",
                "\n",
                "#### Scenario:\n",
                "You want to analyze only the orders with a quantity greater than 2.\n",
                "\n",
                "```python\n",
                "# Question:\n",
                "# Create a new DataFrame 'df_large_orders' containing only the orders with Quantity greater than 2.\n",
                "# Display the new DataFrame.\n",
                "```"
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "cce722df-a534-40a0-927a-19985449c317"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Conditional Indexing and Filtering\n",
                "df_large_orders = df_orders[df_orders['Quantity'] > 2]\n",
                "\n",
                "# Displaying the DataFrame with Large Orders\n",
                "print(\"DataFrame with Orders Quantity > 2:\")\n",
                "print(df_large_orders)"
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "e369539d-4408-4686-9af4-161680705580"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "DataFrame with Orders Quantity > 2:\n   OrderID Product  Quantity  Total_Price\n3      103  Tablet         3          450\n"
                }
            ],
            "execution_count": 19
        }
    ]
}