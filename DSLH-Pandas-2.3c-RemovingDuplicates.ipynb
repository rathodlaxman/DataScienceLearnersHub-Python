{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# **`Data Science Learners Hub`**\n",
                "\n",
                "**Module : Python**\n",
                "\n",
                "**email** : [datasciencelearnershub@gmail.com](mailto:datasciencelearnershub@gmail.com)"
            ],
            "metadata": {
                "azdata_cell_guid": "3ebab10c-a39b-465e-b45e-4f59bacfe138"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## **`#2: DataFrames in Depth`**\n",
                "\n",
                "4. **Creating DataFrames**\n",
                "    \n",
                "    - From lists, dictionaries, and arrays\n",
                "    - Reading data from CSV, Excel, and other formats\n",
                "5. **Basic DataFrame Operations**\n",
                "    \n",
                "    - Inspecting the DataFrame\n",
                "    - Indexing and selecting data\n",
                "    - Descriptive statistics\n",
                "6. **Data Cleaning and Handling Missing Data**\n",
                "    \n",
                "    - Handling missing values\n",
                "    - Dropping or filling missing values\n",
                "    - Removing duplicates"
            ],
            "metadata": {
                "azdata_cell_guid": "d304d0a9-cd0d-45e6-aca4-7fe88f13b4a0"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### **`Removing Duplicates in a DataFrame`**\n",
                "\n",
                "#### **1. Significance of Identifying and Removing Duplicate Rows:**\n",
                "\n",
                "**a. Data Accuracy:**\n",
                "   - Duplicate rows can distort analyses by inflating counts, averages, or other summary statistics. Removing duplicates ensures the accuracy of calculated metrics.\n",
                "\n",
                "**b. Consistent Results:**\n",
                "   - Duplicates can lead to inconsistencies in results, especially in scenarios where aggregated data or distinct counts are essential.\n",
                "\n",
                "**c. Efficient Memory Usage:**\n",
                "   - Datasets with duplicate rows consume more memory. Eliminating duplicates optimizes memory usage and enhances computational efficiency.\n",
                "\n",
                "**d. Meaningful Insights:**\n",
                "   - Duplicate rows may not contribute meaningful insights but can skew results. Removing them ensures a cleaner dataset for analysis.\n",
                "\n",
                "#### Examples of Removing Duplicates:\n",
                "\n",
                "**1. Identifying Duplicate Rows:**\n",
                "```python\n",
                "# Check for duplicate rows based on all columns\n",
                "duplicates = df.duplicated()\n",
                "\n",
                "# Check for duplicate rows based on specific columns\n",
                "duplicates_specific_columns = df.duplicated(subset=['Column1', 'Column2'])\n",
                "```\n",
                "\n",
                "**2. Removing Duplicate Rows:**\n",
                "```python\n",
                "# Remove all duplicate rows, keeping the first occurrence\n",
                "df_no_duplicates = df.drop_duplicates()\n",
                "\n",
                "# Remove duplicate rows based on specific columns, keeping the first occurrence\n",
                "df_no_duplicates_specific_columns = df.drop_duplicates(subset=['Column1', 'Column2'])\n",
                "```\n",
                "\n",
                "#### Considerations:\n",
                "\n",
                "- **Column Selection:**\n",
                "  - Consider the columns relevant to duplicate identification. In some cases, duplicates may only be duplicates when considering specific columns.\n",
                "\n",
                "- **Order Matters:**\n",
                "  - `drop_duplicates()` retains the first occurrence and removes subsequent duplicates. Ensure the order aligns with your analysis goals.\n",
                "\n",
                "- **In-Place vs. New DataFrame:**\n",
                "  - Decide whether to modify the existing DataFrame in-place or create a new one. Choose based on the need to retain the original data.\n",
                "\n",
                "#### Common Mistakes:\n",
                "\n",
                "- **Ignoring Specific Columns:**\n",
                "  - Failing to specify columns during duplicate checking can result in unintended removal of rows that might be duplicates only in certain columns.\n",
                "\n",
                "- **Overlooking Order:**\n",
                "  - If retaining the first occurrence is essential, ensure that the DataFrame is sorted appropriately before using `drop_duplicates()`.\n",
                "\n",
                "- **Inconsistent Usage:**\n",
                "  - Inconsistently applying duplicate removal across different datasets or analyses can lead to inconsistent results.\n",
                "\n",
                "#### Conclusion:\n",
                "\n",
                "Identifying and removing duplicate rows is a crucial step in data cleaning and preprocessing. It enhances the accuracy of analyses, ensures meaningful insights, and optimizes memory usage. Developers should carefully consider the columns involved, the order of removal, and whether to modify the DataFrame in-place when handling duplicates."
            ],
            "metadata": {
                "azdata_cell_guid": "69560ebe-9f83-40c0-8d42-e25bcafe6a43"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### 2. Example:\n",
                "\n",
                "Let's consider a scenario where you have a DataFrame containing data on customer orders, and due to data entry errors or system glitches, there are duplicate entries. We'll explore how to identify and remove these duplicate rows using Pandas."
            ],
            "metadata": {
                "azdata_cell_guid": "bd4f7fbb-36a5-4f81-b410-e7b9c02cf8e6"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Sample order data with duplicate entries\n",
                "order_data = {\n",
                "    'OrderID': [101, 102, 101, 103, 104, 102],\n",
                "    'Product': ['Laptop', 'Smartphone', 'Laptop', 'Tablet', 'Camera', 'Smartphone'],\n",
                "    'Quantity': [2, 1, 1, 3, 1, 1],\n",
                "    'Total_Price': [1200, 800, 1200, 450, 700, 800],\n",
                "}\n",
                "\n",
                "# Creating a DataFrame from the order data\n",
                "df_orders = pd.DataFrame(order_data)\n",
                "\n",
                "# Displaying the original DataFrame\n",
                "print(\"Original Order DataFrame:\")\n",
                "print(df_orders)\n",
                "\n",
                "# Identifying Duplicate Rows\n",
                "duplicates = df_orders.duplicated()\n",
                "\n",
                "# Displaying duplicate rows\n",
                "print(\"\\nDuplicate Rows:\")\n",
                "print(df_orders[duplicates])\n",
                "\n",
                "# Removing Duplicate Rows\n",
                "df_no_duplicates = df_orders.drop_duplicates()\n",
                "\n",
                "# Displaying the DataFrame after removing duplicates\n",
                "print(\"\\nDataFrame after Removing Duplicates:\")\n",
                "print(df_no_duplicates)\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "455a8a1e-201d-4dba-9757-5bf56dc23bb6",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Original Order DataFrame:\n   OrderID     Product  Quantity  Total_Price\n0      101      Laptop         2         1200\n1      102  Smartphone         1          800\n2      101      Laptop         1         1200\n3      103      Tablet         3          450\n4      104      Camera         1          700\n5      102  Smartphone         1          800\n\nDuplicate Rows:\n   OrderID     Product  Quantity  Total_Price\n5      102  Smartphone         1          800\n\nDataFrame after Removing Duplicates:\n   OrderID     Product  Quantity  Total_Price\n0      101      Laptop         2         1200\n1      102  Smartphone         1          800\n2      101      Laptop         1         1200\n3      103      Tablet         3          450\n4      104      Camera         1          700\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 1
        },
        {
            "cell_type": "markdown",
            "source": [
                "In this example:\n",
                "\n",
                "1. **Identifying Duplicate Rows:**\n",
                "   - We use `duplicated()` to identify duplicate rows based on all columns. The result is a boolean series indicating which rows are duplicates.\n",
                "\n",
                "2. **Displaying Duplicate Rows:**\n",
                "   - We use boolean indexing to display the rows that are identified as duplicates.\n",
                "\n",
                "3. **Removing Duplicate Rows:**\n",
                "   - We use `drop_duplicates()` to remove duplicate rows, keeping the first occurrence of each unique row.\n",
                "\n",
                "4. **Displaying Result:**\n",
                "   - We display the DataFrame after removing duplicates to see the cleaned dataset.\n",
                "\n",
                "Adjust the code based on your specific dataset and analysis goals. Understanding the significance of removing duplicates and applying these methods ensures a cleaner and more reliable dataset for further analysis."
            ],
            "metadata": {
                "azdata_cell_guid": "4e8e7214-7de4-46fc-8612-5614d12aead2"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### 3. Considerations or Peculiarities:\n",
                "\n",
                "- **Column Selection:**\n",
                "  - Consider which columns should be considered for identifying duplicates. In some cases, duplicates may only be duplicates when considering specific columns.\n",
                "\n",
                "- **Impact on Analysis:**\n",
                "  - Consider how duplicate rows might impact subsequent analyses. Retaining duplicates might skew results, while removing them ensures a cleaner dataset.\n",
                "\n",
                "#### 4. Common Mistakes:\n",
                "\n",
                "- **Incomplete Duplicate Identification:**\n",
                "  - Not considering all relevant columns during duplicate identification might result in incomplete removal of duplicates.\n",
                "\n",
                "- **Ignoring Context:**\n",
                "  - Failing to understand the context of the data might lead to unintended removal of rows that may be legitimate duplicates.\n",
                "\n",
                "- **Overlooking Order:**\n",
                "  - Forgetting to sort the DataFrame appropriately before using `drop_duplicates()` may lead to unexpected results if order matters.\n",
                "\n",
                "Handling duplicate rows is essential for maintaining data accuracy and ensuring meaningful analyses. Developers should carefully choose columns for duplicate identification, understand the impact of duplicates on analysis, and avoid common mistakes that could compromise data integrity."
            ],
            "metadata": {
                "azdata_cell_guid": "760b5480-3b6d-4da9-bafb-0b86b4caeb82"
            },
            "attachments": {}
        }
    ]
}